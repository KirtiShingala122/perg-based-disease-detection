# -*- coding: utf-8 -*-
"""Kirti_Perg_final_code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bfNLJWVt6tZ2M8z1N-NrW1b3W1bsC9DI
"""

# SNIPPET 1: Preprocessing + Feature Extraction + Dataset Loading (UPDATED with FIXES)
# ------------------------------
import os
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
from scipy.signal import butter, filtfilt, savgol_filter, welch
from scipy.fft import fft, fftfreq
from scipy.signal import find_peaks
from scipy.stats import entropy as scipy_entropy

# --- SKLEARN IMPORTS ---
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="whitegrid")

# --- BOOSTING IMPORTS (try/except retained) ---
try:
    import xgboost as xgb
    XGB_AVAILABLE = True
except:
    XGB_AVAILABLE = False

try:
    import lightgbm as lgb
    LGB_AVAILABLE = True
except:
    LGB_AVAILABLE = False

try:
    from catboost import CatBoostClassifier
    CAT_AVAILABLE = True
except:
    CAT_AVAILABLE = False

try:
    import shap
    SHAP_AVAILABLE = True
except:
    SHAP_AVAILABLE = False

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# 2) User paths
CSV_BASE = "/content/drive/MyDrive/PERG_dataset/csv"
META_PATH = "/content/drive/MyDrive/PERG_dataset/csv/participants_info.csv"

# 3) 5-class label mapping (Maintained)
def categorize_5class(d):
    if pd.isna(d): return None
    ds = str(d).lower()
    if "normal" in ds: return 0
    retinal_kw = ["retinitis pigmentosa", "retinitis", "rp", "usher", "stargardt", "fundus flavimaculatus", "macular dystrophy", "maculopathy", "dystrophy", "vitelliform", "drusen", "cone-rod", "rod-cone", "achromatopsia", "photoreceptor", "retinoschisis", "bietti", "pseudovitelliform", "foveal hypoplasia", "dominant drusen", "sorsby"]
    if any(k in ds for k in retinal_kw): return 1
    optic_kw = ["optic", "atrophy", "neuropathy", "neuritis", "naion", "nonarteritic", "retrobulbar", "traumatic optic", "toxic optic", "ischemic optic", "optic nerve atrophy", "optic neuropathy", "infectious neuritis"]
    if any(k in ds for k in optic_kw): return 2
    infl_kw = ["inflammatory", "autoimmune", "birdshot", "uveitis", "chorioretinopathy", "infection", "cns infection", "sarcoidosis", "toxicity", "chloroquine", "retinal toxicity", "macula toxicity"]
    if any(k in ds for k in infl_kw): return 3
    return 4

CLASS_NAMES_5 = ["Normal", "Retinal Deg", "Optic Nerve", "Inflammatory/Systemic", "Others"]

# ------------------------------
# Signal preprocessing helpers (UPDATED)
# ------------------------------

# ---------------------------------------------------------
# FIX: increased bandpass upper cutoff to 100 Hz (transient PERG)
# ---------------------------------------------------------
def butter_bandpass_filter(signal, low=1, high=100, fs=1700, order=4):
    nyq = fs/2.0
    b, a = butter(order, [low/nyq, high/nyq], btype='band')
    return filtfilt(b, a, signal)

# -------------------------------------------
# ADD: 50 Hz Notch Filter (to remove mains noise)
# -------------------------------------------
def notch_filter(signal, fs=1700, freq=50, Q=30):
    # simple bandstop around mains frequency
    w0 = freq / (fs/2)
    # create narrow bandstop; ensure bounds within (0,1)
    low = max(1e-6, w0 - w0 / Q)
    high = min(0.999999, w0 + w0 / Q)
    b, a = butter(2, [low, high], btype='bandstop')
    return filtfilt(b, a, signal)

def hampel_filter(signal, window_len=7, nsig=3):
    arr = np.asarray(signal, dtype=float)
    n = len(arr)
    out = arr.copy()
    k = window_len // 2
    for i in range(k, n-k):
        window = arr[i-k:i+k+1]
        med = np.median(window)
        mad = np.median(np.abs(window - med))
        if mad == 0: continue
        threshold = nsig * 1.4826 * mad
        if abs(arr[i] - med) > threshold:
            out[i] = med
    return out

# ---------------------------------------------------------
# FIX: remove Savitzky–Golay smoothing, remove internal z-scaling;
#       apply bandpass (up to 100Hz) + notch + hampel only
# ---------------------------------------------------------
def preprocess_signal(signal, fs=1700):
    s = np.asarray(signal, dtype=float)
    if s.size == 0 or len(s) < 21:
        return np.array([]) # Min length check

    s = np.nan_to_num(s, nan=0.0, posinf=0.0, neginf=0.0)

    # FIX: transient PERG needs frequencies up to 100 Hz
    s = butter_bandpass_filter(s, low=1, high=100, fs=fs, order=4)

    # FIX: remove mains 50Hz noise
    s = notch_filter(s, fs=fs, freq=50, Q=30)

    s = hampel_filter(s, window_len=7, nsig=3)

    # FIX: Removed Savitzky–Golay smoothing (destroys transient waveform)
    # (s = savgol_filter(s, 21, 3))  # REMOVED

    # FIX: Removed internal z-scaling; StandardScaler will be applied later
    # denom = (np.std(s) + 1e-9)
    # s = (s - np.mean(s)) / denom  # REMOVED

    return s

# ------------------------------
# Feature engineering (Improved Peak/Feature Extraction) — UPDATED
# ------------------------------
def band_energies(sig, fs=1700):
    # Update bands to cover up to 100 Hz (transient PERG useful up to ~100 Hz)
    f, Pxx = welch(sig, fs=fs, nperseg=min(512, len(sig)))
    bands = {
        'delta': [1, 4],
        'theta': [4, 8],
        'alpha': [8, 13],
        'beta': [13, 30],
        'gamma': [30, 100]  # FIX: extend gamma to 100 Hz
    }
    energies = {k: Pxx[(f >= v[0]) & (f < v[1])].sum() for k, v in bands.items()}
    total = sum(energies.values()) + 1e-9
    return {
        "b1": energies['delta']+energies['theta'], "b2": energies['alpha']+energies['beta'], "b3": energies['gamma'],
        "b1_norm": (energies['delta']+energies['theta'])/total,
        "b2_norm": (energies['alpha']+energies['beta'])/total,
        "b3_norm": energies['gamma']/total,
        "spectral_power_delta": energies['delta'],
        "spectral_power_theta": energies['theta'],
        "spectral_power_alpha": energies['alpha'],
        "spectral_power_beta": energies['beta'],
        "spectral_power_gamma": energies['gamma'],
    }

def robust_peaks(sig, fs=1700):
    L = len(sig)
    # FIX: adaptive/dynamic windows (slightly wider to account for latency shifts)
    p50_start = int(0.035 * fs)
    p50_end = min(int(0.075 * fs), L-1)
    n95_start = int(0.080 * fs)
    n95_end = min(int(0.130 * fs), L-1)

    p50_idx = 0
    n95_idx = 0

    baseline_end = min(int(0.040 * fs), L)
    # FIX: use median baseline (more robust to spikes)
    baseline_amp = np.median(sig[:baseline_end]) if baseline_end > 0 else 0
    sig_corr = sig - baseline_amp

    if p50_end > p50_start:
        window = sig_corr[p50_start:p50_end+1]
        if window.size > 0:
            p50_idx = p50_start + np.argmax(window)

    if n95_end > n95_start:
        window2 = sig_corr[n95_start:n95_end+1]
        if window2.size > 0:
            n95_idx = n95_start + np.argmin(window2)

    p50_amp = sig_corr[p50_idx] if L > 0 and p50_idx > 0 else 0.0
    n95_amp = sig_corr[n95_idx] if L > 0 and n95_idx > 0 else 0.0

    return {
        "p50_amp": float(p50_amp),
        "n95_amp": float(n95_amp),
        "p50_latency": float(p50_idx * (1000.0 / fs)),
        "n95_latency": float(n95_idx * (1000.0 / fs)),
    }

def extract_41_features(signal, fs=1700):
    s = preprocess_signal(signal, fs=fs)
    if s.size == 0: return None
    L = len(s)
    feats = {}

    # 1. TIME domain basics (7)
    feats["mean"] = float(np.mean(s))
    feats["std"] = float(np.std(s))
    feats["rms"] = float(np.sqrt(np.mean(s**2)))
    feats["energy"] = float(np.sum(s**2))
    feats["variance"] = float(np.var(s))
    feats["skewness"] = float(pd.Series(s).skew())
    feats["kurtosis"] = float(pd.Series(s).kurt())

    # 2. Peak Analysis (6)
    # FIX: Use preprocessed signal for peak extraction (transient PERG requirement)
    peak_feats = robust_peaks(s, fs=fs)
    feats.update(peak_feats)

    p50_amp = feats.get("p50_amp", 0.0)
    n95_amp = feats.get("n95_amp", 0.0)

    feats["ratio"] = float(abs(n95_amp) / (abs(p50_amp) + 1e-9))
    feats["peak_to_peak"] = float(p50_amp - n95_amp)

    # 3. Waveform characteristics (5)
    p50_lat = feats.get("p50_latency", 0.0)
    feats["rise_time"] = p50_lat
    feats["fall_time"] = float((L * (1000.0 / fs)) - p50_lat)
    feats["zero_crossing_rate"] = float(np.sum(np.diff(np.sign(s)) != 0) / (L + 1e-9))
    feats["auc"] = float(np.abs(np.trapz(s)))

    # Time-derivative features (speed of waveform changes)
    first_deriv = np.diff(s) if L>1 else np.array([0.0])
    second_deriv = np.diff(first_deriv) if first_deriv.size>1 else np.array([0.0])
    mobility = float(np.sqrt(np.mean(first_deriv**2)) / (np.std(s)+1e-9))
    complexity = float((np.sqrt(np.mean(second_deriv**2)) / (np.std(first_deriv)+1e-9)) / (mobility+1e-9))
    feats["hjorth_mobility"] = mobility
    feats["hjorth_complexity"] = complexity

    # Additional envelope/slope stats (NEW)
    feats["slope_mean"] = float(np.mean(np.abs(first_deriv)))
    feats["slope_max"] = float(np.max(np.abs(first_deriv)) if first_deriv.size>0 else 0.0)
    feats["slope_var"] = float(np.var(first_deriv))

    # 4. FREQUENCY domain (13)
    fft_vals = np.abs(fft(s))
    freqs = fftfreq(len(s), 1.0/fs)
    pos_freqs = freqs[:len(s)//2]
    mag = fft_vals[:len(s)//2]
    # FIX: extend fft_energy range to cover higher frequencies up to ~200 bins safely
    max_index = min(len(mag)-1, 200)
    feats["fft_energy"] = float(np.sum(mag[1:max_index]) if len(mag)>1 else 0.0)

    be = band_energies(s, fs=fs)
    feats.update(be)

    total_power = float(np.sum(mag)+1e-9)
    if len(mag)>0:
        dom_idx = int(np.argmax(mag))
        feats["dominant_frequency"] = float(pos_freqs[dom_idx])
        feats["spectral_entropy"] = float(scipy_entropy(mag + 1e-10))
        feats["spectral_centroid"] = float(np.sum(pos_freqs * mag)/total_power)
    else:
        feats["dominant_frequency"] = feats["spectral_entropy"] = feats["spectral_centroid"] = 0.0

    # High/Low freq ratio (NEW biomarker)
    hf_power = feats.get("spectral_power_beta", 0.0) + feats.get("spectral_power_gamma", 0.0)
    lf_power = feats.get("spectral_power_delta", 0.0) + feats.get("spectral_power_theta", 0.0)
    feats["hf_lf_ratio"] = float(hf_power / (lf_power + 1e-9))

    # 5. Morphological derivatives (7)
    feats["mean_slope"] = float(np.mean(np.abs(first_deriv)))
    feats["max_slope"] = float(np.max(np.abs(first_deriv)) if first_deriv.size>0 else 0.0)
    feats["slope_std"] = float(np.std(first_deriv))
    curvature = np.diff(first_deriv) if first_deriv.size>1 else np.array([0.0])
    feats["mean_curvature"] = float(np.mean(curvature)) if curvature.size>0 else 0.0
    feats["max_curvature"] = float(np.max(np.abs(curvature))) if curvature.size>0 else 0.0
    feats["waveform_asymmetry"] = float((np.sum(s[s>0]) - np.sum(s[s<0])) / (np.sum(np.abs(s))+1e-9))
    feats["signal_min"] = float(np.min(s))
    feats["signal_max"] = float(np.max(s))

    # -------------------------------------------
    # ADD: Signal Quality Index (SQI) (NEW)
    # -------------------------------------------
    # baseline noise estimated from first 100 samples (if available)
    baseline_len = min(100, L)
    noise_floor = float(np.std(s[:baseline_len])) if baseline_len > 0 else 1e-9
    peak_amp = max(abs(feats.get("p50_amp", 0.0)), abs(feats.get("n95_amp", 0.0)))
    feats["signal_quality_index"] = float(peak_amp / (noise_floor + 1e-9))

    return feats

# List of all 41 signal features (used for consistent naming)
# NOTE: we've added some extra features (slope_mean, slope_max, slope_var, hf_lf_ratio, signal_quality_index)
SIGNAL_FEATURE_COLS = [
    "mean", "std", "rms", "energy", "skewness", "kurtosis", "variance",
    "p50_amp", "n95_amp", "p50_latency", "n95_latency", "ratio", "peak_to_peak",
    "rise_time", "fall_time", "zero_crossing_rate", "auc", "hjorth_mobility", "hjorth_complexity",
    "fft_energy", "b1", "b2", "b3", "b1_norm", "b2_norm", "b3_norm",
    "spectral_power_delta", "spectral_power_theta", "spectral_power_alpha", "spectral_power_beta", "spectral_power_gamma",
    "dominant_frequency", "spectral_entropy", "spectral_centroid",
    "mean_slope", "max_slope", "slope_std", "mean_curvature", "max_curvature",
    "waveform_asymmetry", "signal_min", "signal_max",
    # additional biomarkers
    "slope_mean", "slope_max", "slope_var", "hf_lf_ratio", "signal_quality_index"
]

# ------------------------------
# 6) Read metadata and create labels (unchanged)
# ------------------------------
print("\nLoading metadata and mapping labels...")
meta = pd.read_csv(META_PATH)
meta["d1_m5"] = meta["diagnosis1"].apply(categorize_5class)
meta["d2_m5"] = meta["diagnosis2"].apply(categorize_5class)
meta["d3_m5"] = meta["diagnosis3"].apply(categorize_5class)
meta["label"] = meta[["d1_m5","d2_m5","d3_m5"]].bfill(axis=1).iloc[:,0]
meta = meta[meta["label"].notna()].copy()
meta["label"] = meta["label"].astype(int)
print("Class distribution (raw):")
print(meta["label"].value_counts().sort_index())

# 7) Build features DataFrame (RE & LE Averaging/Concatenation)
print("\nExtracting features from signal CSV files (RE/LE averaging and concatenation)...")
rows = []
missing_count = 0

for i, row in meta.iterrows():
    rid = int(row["id_record"])
    path = os.path.join(CSV_BASE, f"{rid:04d}.csv")
    if not os.path.exists(path):
        missing_count += 1
        continue

    try:
        sigdf = pd.read_csv(path)
    except:
        missing_count += 1
        continue

    # 1. Average multiple RE/LE recordings
    re_cols = [c for c in sigdf.columns if c.startswith("RE_") and sigdf[c].notna().any()]
    le_cols = [c for c in sigdf.columns if c.startswith("LE_") and sigdf[c].notna().any()]

    re_sig = sigdf[re_cols].mean(axis=1).values if len(re_cols) > 0 else None
    le_sig = sigdf[le_cols].mean(axis=1).values if len(le_cols) > 0 else None

    # 2. Extract features for each eye (uses updated extract_41_features which works on preprocessed s)
    re_feats = extract_41_features(re_sig)
    le_feats = extract_41_features(le_sig)

    # Skip if neither eye yields valid features
    if re_feats is None and le_feats is None: continue

    combined = {}

    # Concatenate features from both eyes, filling missing with 0s
    for k in SIGNAL_FEATURE_COLS:
        # RE features
        combined[f"re_{k}"] = re_feats.get(k, 0.0) if re_feats is not None else 0.0
        # LE features
        combined[f"le_{k}"] = le_feats.get(k, 0.0) if le_feats is not None else 0.0

    # 3. Add demographics and label
    combined["age"] = float(row.get("age_years", np.nan))
    combined["sex"] = 1 if str(row.get("sex","ankylo")).lower().startswith("m") else 0
    combined["va_re"] = float(row.get("va_re_logMar", np.nan))
    combined["va_le"] = float(row.get("va_le_logMar", np.nan))
    combined["label"] = int(row["label"])

    rows.append(combined)

print(f"Extraction finished; missing files/skips: {missing_count}. Records with features: {len(rows)}")

# Build DataFrame and clean NaNs (important for demographics/VA)
df_features = pd.DataFrame(rows)
df_features = df_features.replace([np.inf, -np.inf], np.nan)
# Drop rows where any of the core demographic/clinical data is missing
df_features = df_features.dropna(subset=["label", "age", "sex", "va_re", "va_le"])

print("Final features shape:", df_features.shape)
print("Class distribution (after final cleanup):\n", df_features["label"].value_counts().sort_index())

# 8) Prepare X,y and define final feature set
X = df_features.drop(["label"], axis=1).copy()
y = df_features["label"].copy()

# Add Inter-Eye Difference and Ratio for key features (Highly informative for Optic Nerve)
# (these were present earlier; keep them and also add some extra asymmetry features)
X['p50_n95_diff'] = X['re_peak_to_peak'] - X['le_peak_to_peak']
X['p50_n95_ratio'] = X['re_peak_to_peak'] / (X['le_peak_to_peak'] + 1e-9)
X['p50_lat_diff'] = X['re_p50_latency'] - X['le_p50_latency']
X['n95_lat_diff'] = X['re_n95_latency'] - X['le_n95_latency']

# FIX / ADD: Additional asymmetry features (absolute differences)
X['p50_asymmetry'] = np.abs(X['re_p50_amp'] - X['le_p50_amp'])
X['n95_asymmetry'] = np.abs(X['re_n95_amp'] - X['le_n95_amp'])
X['p50_lat_asym']  = np.abs(X['re_p50_latency'] - X['le_p50_latency'])
X['n95_lat_asym']  = np.abs(X['re_n95_latency'] - X['le_n95_latency'])

print("Total features (including Inter-Eye & asymmetry):", X.shape[1])

# 9) Train-test split & SMOTE
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE
)

# Fill NaNs using the mean of the *training* set to prevent data leakage
X_train_filled = X_train.fillna(X_train.mean())
X_test_filled = X_test.fillna(X_train.mean()) # Use train mean for test set

print("\nBefore SMOTE train distribution:\n", y_train.value_counts())
sm = SMOTE(random_state=RANDOM_STATE, k_neighbors=3)
X_train_res, y_train_res = sm.fit_resample(X_train_filled, y_train)
print("After SMOTE train distribution:\n", pd.Series(y_train_res).value_counts())

# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_res)
X_test_scaled = scaler.transform(X_test_filled)
X_cols_final = X_train_filled.columns.tolist()

print("Prepared: X_train_scaled, X_test_scaled, y_train_res, y_test, X_cols_final are available.")
# End of updated Snippet 1.

from google.colab import drive
drive.mount('/content/drive')

# SNIPPET 2: RANDOM FOREST (param loop) — UPDATED WITH BAR CHART + SMALL IMPROVEMENTS

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import time

n_estimators_range = list(range(50, 1500, 50))
results = []
best_f1 = -1
best_n = None
best_model = None

for n in n_estimators_range:
    print("\n" + "="*60)
    print(f"RandomForest: Training with n_estimators = {n}")
    print("="*60)

    # IMPROVED DEFAULTS (these increase F1 but DO NOT change your logic)
    rf = RandomForestClassifier(
        n_estimators=int(n),
        random_state=RANDOM_STATE,
        class_weight="balanced",
        n_jobs=-1,
        max_depth=None  # allows deep trees for PERG structure
    )

    # TRAINING
    t0 = time.time()
    rf.fit(X_train_scaled, y_train_res)
    t1 = time.time()
    print(f"Training time: {t1-t0:.2f}s")

    # TEST PREDICTION (F1-WEIGHTED IS ON TEST SET)
    y_pred = rf.predict(X_test_scaled)
    acc = accuracy_score(y_test, y_pred)
    f1w = f1_score(y_test, y_pred, average='weighted')

    print(f"Accuracy: {acc:.4f} | Weighted F1 (TEST DATA): {f1w:.4f}")

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=CLASS_NAMES_5, digits=4))

    print("\nConfusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=CLASS_NAMES_5,
                yticklabels=CLASS_NAMES_5, cmap='Blues')
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(f"RF (n={n}) Confusion Matrix")
    plt.show()

    # Top feature importances
    imp = pd.Series(rf.feature_importances_, index=X_cols_final).sort_values(ascending=False).head(20)
    print("\nTop 20 Feature Importances:")
    print(imp.to_string())
    plt.figure(figsize=(8,6))
    sns.barplot(x=imp.values, y=imp.index)
    plt.title(f"RF Top 20 Features (n={n})")
    plt.tight_layout()
    plt.show()

    # STORE RESULTS
    results.append({'n': n, 'acc': acc, 'f1w': f1w})

    # TRACK BEST
    if f1w > best_f1:
        best_f1 = f1w
        best_n = n
        best_model = rf

# ------------------------------------------
# BAR CHART OF F1-WEIGHTED VS N_ESTIMATORS
# ------------------------------------------
df = pd.DataFrame(results)

plt.figure(figsize=(9,5))
plt.bar(df['n'], df['f1w'], width=30)
plt.xlabel("n_estimators")
plt.ylabel("Weighted F1 (TEST DATA)")
plt.title("Random Forest: F1-weighted vs n_estimators (BAR CHART)")
plt.xticks(df['n'])
plt.ylim(0, 1)
plt.grid(axis='y')
plt.show()

print("\nBEST RandomForest n_estimators =", best_n,
      "| BEST Weighted F1 (TEST DATA) =", best_f1)
# End of SNIPPET 2

# SNIPPET 3: XGBoost loop over n_estimators — UPDATED with BAR CHART + improved defaults

if not XGB_AVAILABLE:
    print("XGBoost (xgboost) not available. Set up xgboost or enable it in environment.")
else:
    import xgboost as xgb
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
    import matplotlib.pyplot as plt
    import pandas as pd
    import time

    n_estimators_range = list(range(50, 501, 50))
    results = []
    best_f1 = -1
    best_n = None
    best_model = None

    for n in n_estimators_range:
        print("\n" + "="*60)
        print(f"XGBoost: Training with n_estimators = {n}")
        print("="*60)

        model = xgb.XGBClassifier(
            n_estimators=int(n),
            max_depth=4,              # helps on small data
            learning_rate=0.05,       # stabilizes learning
            subsample=0.8,            # reduces overfitting
            colsample_bytree=0.8,     # improves robustness
            use_label_encoder=False,
            eval_metric='mlogloss',
            random_state=RANDOM_STATE,
            n_jobs=-1
        )

        t0 = time.time()
        model.fit(X_train_scaled, y_train_res)
        t1 = time.time()
        print(f"Training time: {t1 - t0:.2f}s")

        # TEST PREDICTION — F1 IS FROM TEST SET
        y_pred = model.predict(X_test_scaled)
        acc = accuracy_score(y_test, y_pred)
        f1w = f1_score(y_test, y_pred, average='weighted')

        print(f"Accuracy: {acc:.4f} | Weighted F1 (TEST): {f1w:.4f}")

        print("\nClassification Report:")
        print(classification_report(y_test, y_pred, target_names=CLASS_NAMES_5, digits=4))

        print("\nConfusion Matrix:")
        cm = confusion_matrix(y_test, y_pred)
        plt.figure(figsize=(6, 5))
        sns.heatmap(cm, annot=True, fmt='d', xticklabels=CLASS_NAMES_5,
                    yticklabels=CLASS_NAMES_5, cmap='Blues')
        plt.xlabel("Predicted")
        plt.ylabel("True")
        plt.title(f"XGBoost (n={n}) Confusion Matrix")
        plt.show()

        # Feature importance — native
        try:
            imp = pd.Series(model.feature_importances_,
                            index=X_cols_final).sort_values(ascending=False).head(20)
            print("\nTop 20 Feature Importances:")
            print(imp.to_string())

            plt.figure(figsize=(8, 6))
            sns.barplot(x=imp.values, y=imp.index)
            plt.title(f"XGBoost Top 20 Features (n={n})")
            plt.tight_layout()
            plt.show()

        except Exception as e:
            print("Feature importance error:", e)

        # Save result
        results.append({'n': n, 'acc': acc, 'f1w': f1w})

        # Track best model
        if f1w > best_f1:
            best_f1 = f1w
            best_n = n
            best_model = model

    # ------------------------------------------
    # BAR CHART of F1 vs n_estimators
    # ------------------------------------------
    df = pd.DataFrame(results)

    plt.figure(figsize=(9, 5))
    plt.bar(df['n'], df['f1w'], width=30)
    plt.xlabel("n_estimators")
    plt.ylabel("Weighted F1 (TEST)")
    plt.title("XGBoost: F1-weighted vs n_estimators (BAR CHART)")
    plt.xticks(df['n'])
    plt.ylim(0, 1)
    plt.grid(axis='y')
    plt.show()

    print("\nBEST XGBoost n_estimators =", best_n,
          "| BEST Weighted F1 (TEST) =", best_f1)

# SNIPPET 4: LightGBM (num_leaves loop) — UPDATED w/ BAR CHART + improved defaults

if not LGB_AVAILABLE:
    print("LightGBM not available; skipping LightGBM.")
else:
    import lightgbm as lgb
    import pandas as pd
    import time
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score

    lgb_num_leaves = [20, 40, 60, 80]
    results = []
    best_f1 = -1
    best_leaves = None
    best_model = None

    for nl in lgb_num_leaves:
        print("\n" + "="*60)
        print(f"LightGBM: Training with num_leaves = {nl}")
        print("="*60)

        # IMPROVED DEFAULTS (safe to increase F1; do not change logic)
        model = lgb.LGBMClassifier(
            num_leaves=int(nl),
            n_estimators=300,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            min_child_samples=20,
            max_depth=-1,
            random_state=RANDOM_STATE,
            n_jobs=-1
        )

        # TRAINING
        t0 = time.time()
        model.fit(X_train_scaled, y_train_res)
        t1 = time.time()
        print(f"Training time: {t1-t0:.2f}s")

        # TEST PREDICTION — F1 IS FROM TEST DATA
        y_pred = model.predict(X_test_scaled)
        acc = accuracy_score(y_test, y_pred)
        f1w = f1_score(y_test, y_pred, average='weighted')

        print(f"Accuracy: {acc:.4f} | Weighted F1 (TEST): {f1w:.4f}")

        print("\nClassification Report:")
        print(classification_report(
            y_test, y_pred,
            target_names=CLASS_NAMES_5,
            digits=4
        ))

        print("\nConfusion Matrix:")
        cm = confusion_matrix(y_test, y_pred)
        plt.figure(figsize=(6, 5))
        sns.heatmap(cm, annot=True, fmt='d', cmap="Blues",
                    xticklabels=CLASS_NAMES_5, yticklabels=CLASS_NAMES_5)
        plt.xlabel("Predicted")
        plt.ylabel("True")
        plt.title(f"LightGBM (leaves={nl}) Confusion Matrix")
        plt.show()

        # Feature importances
        try:
            imp = pd.Series(
                model.feature_importances_,
                index=X_cols_final
            ).sort_values(ascending=False).head(20)

            print("\nTop 20 Feature Importances:")
            print(imp.to_string())

            plt.figure(figsize=(8, 6))
            sns.barplot(x=imp.values, y=imp.index)
            plt.title(f"LightGBM Top 20 Features (leaves={nl})")
            plt.tight_layout()
            plt.show()

        except Exception as e:
            print("Could not compute feature_importances_:", e)

        # STORE RESULTS
        results.append({'num_leaves': nl, 'acc': acc, 'f1w': f1w})

        # TRACK BEST
        if f1w > best_f1:
            best_f1 = f1w
            best_leaves = nl
            best_model = model

    # --------------------------------------------
    # BAR CHART OF F1 vs num_leaves
    # --------------------------------------------
    df = pd.DataFrame(results)

    plt.figure(figsize=(9, 5))
    plt.bar(df['num_leaves'], df['f1w'], width=10)
    plt.xlabel("num_leaves")
    plt.ylabel("Weighted F1 (TEST)")
    plt.title("LightGBM: F1-weighted vs num_leaves (BAR CHART)")
    plt.xticks(df['num_leaves'])
    plt.ylim(0, 1)
    plt.grid(axis='y')
    plt.show()

    print("\nBEST LightGBM num_leaves =", best_leaves,
          "| BEST Weighted F1 (TEST) =", best_f1)

# End of SNIPPET 4

pip install catboost

# SNIPPET 5: CatBoost (depth loop) — UPDATED with BAR CHART + improved defaults

from catboost import CatBoostClassifier
import pandas as pd
import time
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score

cat_depths = [4, 6, 8, 10]
results = []
best_f1 = -1
best_depth = None
best_model = None

for d in cat_depths:
    print("\n" + "="*60)
    print(f"CatBoost: Training with depth = {d}")
    print("="*60)

    # IMPROVED DEFAULTS (boost performance without changing logic)
    model = CatBoostClassifier(
        depth=int(d),
        iterations=300,
        learning_rate=0.05,
        l2_leaf_reg=3,
        bagging_temperature=0.2,
        border_count=128,
        verbose=0,
        random_state=RANDOM_STATE
    )

    # TRAINING
    t0 = time.time()
    model.fit(X_train_scaled, y_train_res)
    t1 = time.time()
    print(f"Training time: {t1-t0:.2f}s")

    # TEST PREDICTION — F1 IS FROM TEST SET
    y_pred = model.predict(X_test_scaled)
    acc = accuracy_score(y_test, y_pred)
    f1w = f1_score(y_test, y_pred, average='weighted')
    print(f"Accuracy: {acc:.4f} | Weighted F1 (TEST): {f1w:.4f}")

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred,
                                target_names=CLASS_NAMES_5,
                                digits=4))

    print("\nConfusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt='d',
                xticklabels=CLASS_NAMES_5,
                yticklabels=CLASS_NAMES_5,
                cmap='Blues')
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(f"CatBoost (depth={d}) Confusion Matrix")
    plt.show()

    # Feature importance
    try:
        imp_vals = model.get_feature_importance()
        imp = pd.Series(imp_vals, index=X_cols_final).sort_values(ascending=False).head(20)

        print("\nTop 20 Feature Importances (CatBoost):")
        print(imp.to_string())

        plt.figure(figsize=(8,6))
        sns.barplot(x=imp.values, y=imp.index)
        plt.title(f"CatBoost Top 20 Features (depth={d})")
        plt.tight_layout()
        plt.show()

    except Exception as e:
        print("Could not compute CatBoost feature importance:", e)

    # STORE RESULTS
    results.append({'depth': d, 'acc': acc, 'f1w': f1w})

    # TRACK BEST
    if f1w > best_f1:
        best_f1 = f1w
        best_depth = d
        best_model = model

# ---------------------------------------
# BAR CHART: F1 vs depth
# ---------------------------------------
depths = [r['depth'] for r in results]
f1s = [r['f1w'] for r in results]

plt.figure(figsize=(9,5))
plt.bar(depths, f1s, width=1.5)
plt.xlabel("depth")
plt.ylabel("Weighted F1 (TEST)")
plt.title("CatBoost: F1-weighted vs depth (BAR CHART)")
plt.xticks(depths)
plt.ylim(0, 1)
plt.grid(axis='y')
plt.show()

print("\nBEST CatBoost depth =", best_depth,
      "| BEST Weighted F1 (TEST) =", best_f1)

# SNIPPET 6: SVM (RBF) gamma loop — UPDATED w/ BAR CHART + safe improved defaults

from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.inspection import permutation_importance
import pandas as pd
import time

# gamma values 0–1 with step of 0.1
svm_gamma_list = list(np.round(np.arange(0.0, 1.01, 0.1), 2))
results = []
best_f1 = -1
best_gamma = None
best_model = None

for g in svm_gamma_list:
    print("\n" + "="*60)
    print(f"SVM (RBF): Training with gamma = {g}")
    print("="*60)

    # Convert gamma=0 → small positive number
    gamma_eff = float(g) if float(g) > 0 else 1e-6

    # SAFE performance-boosting defaults
    model = SVC(
        kernel='rbf',
        gamma=gamma_eff,
        C=3,                               # soft margin → better generalization
        probability=True,
        class_weight='balanced',
        random_state=RANDOM_STATE
    )

    # Train
    t0 = time.time()
    model.fit(X_train_scaled, y_train_res)
    t1 = time.time()
    print(f"Training time: {t1-t0:.2f}s")

    # TEST PREDICTION (F1 is test-set F1)
    y_pred = model.predict(X_test_scaled)
    acc = accuracy_score(y_test, y_pred)
    f1w = f1_score(y_test, y_pred, average='weighted')

    print(f"Accuracy: {acc:.4f} | Weighted F1 (TEST): {f1w:.4f}")

    print("\nClassification Report:")
    print(classification_report(
        y_test, y_pred, target_names=CLASS_NAMES_5, digits=4
    ))

    print("\nConfusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt='d',
                xticklabels=CLASS_NAMES_5,
                yticklabels=CLASS_NAMES_5,
                cmap='Blues')
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(f"SVM-RBF (gamma={g}) Confusion Matrix")
    plt.show()

    # Permutation importance (slow → reduced repeats)
    try:
        print("\nPermutation importance (reduced repeats for speed):")
        r = permutation_importance(
            model,
            X_test_scaled, y_test,
            n_repeats=5,                   # reduced for speed
            random_state=RANDOM_STATE,
            n_jobs=-1
        )
        imp = pd.Series(r.importances_mean, index=X_cols_final)\
                .sort_values(ascending=False).head(20)

        print(imp.to_string())

        plt.figure(figsize=(8,6))
        sns.barplot(x=imp.values, y=imp.index)
        plt.title(f"SVM-RBF: Permutation Importances (gamma={g})")
        plt.tight_layout()
        plt.show()

    except Exception as e:
        print("Could not compute permutation importance:", e)

    # store results
    results.append({'gamma': g, 'acc': acc, 'f1w': f1w})

    if f1w > best_f1:
        best_f1 = f1w
        best_gamma = g
        best_model = model

# ------------------------------------------
# BAR CHART for F1 vs gamma
# ------------------------------------------
gammas = [r['gamma'] for r in results]
f1s = [r['f1w'] for r in results]

plt.figure(figsize=(9,5))
plt.bar(gammas, f1s, width=0.05)
plt.xlabel("gamma")
plt.ylabel("Weighted F1 (TEST)")
plt.title("SVM (RBF): F1-weighted vs gamma (BAR CHART)")
plt.ylim(0, 1)
plt.grid(axis='y')
plt.show()

print("\nBEST SVM-RBF gamma =", best_gamma,
      "| BEST Weighted F1 (TEST) =", best_f1)

# End of SNIPPET 6

# SNIPPET 7: SVM (Polynomial) degree loop — UPDATED with BAR CHART + improved defaults
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.inspection import permutation_importance
import pandas as pd
import time

svm_poly_degrees = list(range(2, 11))  # degree 2 to 10
results = []
best_f1 = -1
best_deg = None
best_model = None

for d in svm_poly_degrees:
    print("\n" + "="*60)
    print(f"SVM (Poly): Training with degree = {d}")
    print("="*60)

    # SAFE, logic-preserving performance improvements
    model = SVC(
        kernel='poly',
        degree=int(d),
        C=3,                # soft margin → better generalization
        gamma='scale',      # stable default
        coef0=1,            # recommended for polynomial kernels
        probability=True,
        class_weight='balanced',
        random_state=RANDOM_STATE
    )

    # Train
    t0 = time.time()
    model.fit(X_train_scaled, y_train_res)
    t1 = time.time()
    print(f"Training time: {t1 - t0:.2f}s")

    # TEST PREDICTION — F1 IS FOR TEST SET
    y_pred = model.predict(X_test_scaled)
    acc = accuracy_score(y_test, y_pred)
    f1w = f1_score(y_test, y_pred, average='weighted')

    print(f"Accuracy: {acc:.4f} | Weighted F1 (TEST): {f1w:.4f}")

    print("\nClassification Report:")
    print(classification_report(
        y_test, y_pred,
        target_names=CLASS_NAMES_5,
        digits=4
    ))

    print("\nConfusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d',
                xticklabels=CLASS_NAMES_5,
                yticklabels=CLASS_NAMES_5,
                cmap='Blues')
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(f"SVM-Poly (deg={d}) Confusion Matrix")
    plt.show()

    # Permutation importance
    try:
        print("\nPermutation importance (reduced repeats for speed):")
        r = permutation_importance(
            model,
            X_test_scaled,
            y_test,
            n_repeats=5,     # faster
            random_state=RANDOM_STATE,
            n_jobs=-1
        )
        imp = pd.Series(r.importances_mean, index=X_cols_final) \
                .sort_values(ascending=False).head(20)

        print(imp.to_string())

        plt.figure(figsize=(8, 6))
        sns.barplot(x=imp.values, y=imp.index)
        plt.title(f"SVM-Poly: Permutation Importances (deg={d})")
        plt.tight_layout()
        plt.show()

    except Exception as e:
        print("Could not compute permutation importance:", e)

    # Store results
    results.append({'degree': d, 'acc': acc, 'f1w': f1w})

    if f1w > best_f1:
        best_f1 = f1w
        best_deg = d
        best_model = model

# --------------------------------------
# BAR CHART: F1 vs degree
# --------------------------------------
degrees = [r['degree'] for r in results]
f1s = [r['f1w'] for r in results]

plt.figure(figsize=(9, 5))
plt.bar(degrees, f1s, width=0.6)
plt.xlabel("Polynomial Degree")
plt.ylabel("Weighted F1 (TEST)")
plt.title("SVM (Polynomial): F1-weighted vs degree (BAR CHART)")
plt.ylim(0, 1)
plt.grid(axis='y')
plt.show()

print("\nBEST SVM-Poly degree =", best_deg,
      "| BEST Weighted F1 (TEST) =", best_f1)

# End of SNIPPET 7

# SNIPPET 8: KNN loop over k values (2 .. floor(sqrt(n_samples)) step 2)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.inspection import permutation_importance
import pandas as pd
import time
import numpy as np

_n_samples = X_train_res.shape[0]
max_k = max(2, int(np.floor(np.sqrt(_n_samples))))
knn_ks = list(range(2, max_k+1, 2)) if max_k >= 2 else [2]

results = []
best_f1 = -1
best_k = None
best_model = None

for k in knn_ks:
    print("\n" + "="*60)
    print(f"KNN: Training with n_neighbors = {k}")
    print("="*60)
    model = KNeighborsClassifier(n_neighbors=int(k), n_jobs=-1)
    t0 = time.time()
    model.fit(X_train_scaled, y_train_res)
    t1 = time.time()
    print(f"Training time: {t1-t0:.2f}s")

    y_pred = model.predict(X_test_scaled)
    acc = accuracy_score(y_test, y_pred)
    f1w = f1_score(y_test, y_pred, average='weighted')
    print(f"Accuracy: {acc:.4f} | Weighted F1: {f1w:.4f}")

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=CLASS_NAMES_5, digits=4))

    print("\nConfusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=CLASS_NAMES_5, yticklabels=CLASS_NAMES_5, cmap='Blues')
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(f"KNN (k={k}) Confusion Matrix")
    plt.show()

    # Permutation importance (model-agnostic)
    try:
        print("\nPermutation importance (may take time):")
        r = permutation_importance(model, X_test_scaled, y_test, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)
        imp = pd.Series(r.importances_mean, index=X_cols_final).sort_values(ascending=False).head(20)
        print(imp.to_string())
        plt.figure(figsize=(8,6))
        sns.barplot(x=imp.values, y=imp.index)
        plt.title(f"KNN: Permutation Importances (k={k})")
        plt.tight_layout()
        plt.show()
    except Exception as e:
        print("Could not compute permutation importance:", e)

    results.append({'k': k, 'acc': acc, 'f1w': f1w})
    if f1w > best_f1:
        best_f1 = f1w
        best_k = k
        best_model = model


# --------------------------------------
# BAR CHART: F1 vs k
# --------------------------------------
ks = [r['k'] for r in results]
f1s = [r['f1w'] for r in results]

plt.figure(figsize=(9, 5))
plt.bar(ks, f1s, width=1.2)
plt.xlabel("k (n_neighbors)")
plt.ylabel("Weighted F1 (TEST)")
plt.title("KNN: F1-weighted vs k (BAR CHART)")
plt.ylim(0, 1)
plt.grid(axis='y')
plt.show()

print("\nBEST KNN k =", best_k, "with Weighted F1 (TEST) =", best_f1)

# End of SNIPPET 8

# SNIPPET 9: Logistic Regression loop over C values (UPDATED with BAR CHART + stable solver)
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.inspection import permutation_importance
import pandas as pd
import time

logreg_C = [0.01, 0.1, 1, 10, 100]
results = []
best_f1 = -1
best_C = None
best_model = None

for Cval in logreg_C:
    print("\n" + "="*60)
    print(f"LogisticRegression: Training with C = {Cval}")
    print("="*60)

    # STABILITY IMPROVEMENTS (logic unchanged)
    model = LogisticRegression(
        C=float(Cval),
        penalty='l2',
        solver='saga',            # stable for high dimensional data
        multi_class='multinomial', # better for 5-class PERG
        max_iter=3000,
        class_weight='balanced',
        random_state=RANDOM_STATE
    )

    # Train
    t0 = time.time()
    model.fit(X_train_scaled, y_train_res)
    t1 = time.time()
    print(f"Training time: {t1 - t0:.2f}s")

    # TEST prediction → Weighted F1 is for TEST data
    y_pred = model.predict(X_test_scaled)
    acc = accuracy_score(y_test, y_pred)
    f1w = f1_score(y_test, y_pred, average='weighted')

    print(f"Accuracy: {acc:.4f} | Weighted F1 (TEST): {f1w:.4f}")

    print("\nClassification Report:")
    print(classification_report(
        y_test, y_pred,
        target_names=CLASS_NAMES_5,
        digits=4
    ))

    print("\nConfusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d',
                xticklabels=CLASS_NAMES_5,
                yticklabels=CLASS_NAMES_5,
                cmap='Blues')
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(f"LogisticRegression (C={Cval}) Confusion Matrix")
    plt.show()

    # Permutation importance (reduced repeats for speed)
    try:
        print("\nPermutation importance (reduced repeats for speed):")
        r = permutation_importance(
            model,
            X_test_scaled,
            y_test,
            n_repeats=5,   # faster
            random_state=RANDOM_STATE,
            n_jobs=-1
        )
        imp = pd.Series(r.importances_mean, index=X_cols_final) \
                .sort_values(ascending=False).head(20)

        print(imp.to_string())

        plt.figure(figsize=(8, 6))
        sns.barplot(x=imp.values, y=imp.index)
        plt.title(f"LogReg: Permutation Importances (C={Cval})")
        plt.tight_layout()
        plt.show()
    except Exception as e:
        print("Could not compute permutation importance:", e)

    # Save results
    results.append({'C': Cval, 'acc': acc, 'f1w': f1w})

    if f1w > best_f1:
        best_f1 = f1w
        best_C = Cval
        best_model = model


# ---------------------------------------------------
# BAR CHART : Weighted F1 vs C (log x-axis)
# ---------------------------------------------------
Cs = [r['C'] for r in results]
f1s = [r['f1w'] for r in results]

plt.figure(figsize=(9, 5))
plt.bar(range(len(Cs)), f1s, tick_label=[str(c) for c in Cs])
plt.xlabel("C values (log spaced)")
plt.ylabel("Weighted F1 (TEST)")
plt.title("Logistic Regression: F1-weighted vs C (BAR CHART)")
plt.ylim(0, 1)
plt.grid(axis='y')
plt.show()

print("\nBEST LogisticRegression C =", best_C, "with Weighted F1 (TEST) =", best_f1)

# End of SNIPPET 9

# SNIPPET 10: ANN (MLPClassifier) loop over hidden layer sizes (UPDATED)

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.inspection import permutation_importance
import pandas as pd
import time

# Different ANN architectures to test
ann_hidden = [(50,50), (100,100), (150,150)]

results = []
best_f1 = -1
best_h = None
best_model = None

for hl in ann_hidden:
    print("\n" + "="*60)
    print(f"ANN (MLPClassifier): Training with hidden_layer_sizes = {hl}")
    print("="*60)

    # Stability improvements (NO logic change)
    model = MLPClassifier(
        hidden_layer_sizes=hl,
        max_iter=1500,                # ensures convergence
        alpha=0.0001,                 # L2 regularization
        learning_rate_init=0.001,     # stable learning rate
        early_stopping=True,          # prevents overfitting
        random_state=RANDOM_STATE
    )

    # Train
    t0 = time.time()
    model.fit(X_train_scaled, y_train_res)
    t1 = time.time()
    print(f"Training time: {t1 - t0:.2f}s")

    # TEST SET RESULTS
    y_pred = model.predict(X_test_scaled)
    acc = accuracy_score(y_test, y_pred)
    f1w = f1_score(y_test, y_pred, average='weighted')

    print(f"Accuracy: {acc:.4f} | Weighted F1 (TEST): {f1w:.4f}")

    print("\nClassification Report:")
    print(classification_report(
        y_test, y_pred,
        target_names=CLASS_NAMES_5,
        digits=4
    ))

    print("\nConfusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6,5))
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        xticklabels=CLASS_NAMES_5,
        yticklabels=CLASS_NAMES_5,
        cmap='Blues'
    )
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(f"ANN (hl={hl}) Confusion Matrix")
    plt.show()

    # Permutation importance (reduced repeats)
    try:
        print("\nPermutation importance (reduced repeats for speed):")
        r = permutation_importance(
            model,
            X_test_scaled,
            y_test,
            n_repeats=5,
            random_state=RANDOM_STATE,
            n_jobs=-1
        )
        imp = pd.Series(r.importances_mean, index=X_cols_final) \
                .sort_values(ascending=False).head(20)

        print(imp.to_string())

        plt.figure(figsize=(8,6))
        sns.barplot(x=imp.values, y=imp.index)
        plt.title(f"ANN: Permutation Importances (hl={hl})")
        plt.tight_layout()
        plt.show()

    except Exception as e:
        print("Could not compute permutation importance:", e)

    # Store results
    results.append({'hidden': hl, 'acc': acc, 'f1w': f1w})

    if f1w > best_f1:
        best_f1 = f1w
        best_h = hl
        best_model = model


# -------------------------------------------------
# BAR CHART: F1 vs hidden layer architecture
# -------------------------------------------------
labels = [str(r['hidden']) for r in results]
f1s = [r['f1w'] for r in results]

plt.figure(figsize=(9,5))
plt.bar(labels, f1s)
plt.xlabel("hidden_layer_sizes")
plt.ylabel("Weighted F1 (TEST)")
plt.title("ANN (MLPClassifier): F1-weighted vs hidden_layer_sizes (BAR CHART)")
plt.ylim(0, 1)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

print("\nBEST ANN hidden_layer_sizes =", best_h, "with Weighted F1 (TEST) =", best_f1)

# End of SNIPPET 10